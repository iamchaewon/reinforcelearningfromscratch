
## Chapter 5. 몬테카를로법
---
### 5.1 몬테카를로법 기초
개념: 환경 모델($P, R$)을 모르는 상황에서 에이전트의 **직접적인 경험**을 통해 가치 함수를 추정하는 모델-프리(Model-Free) 학습 방식입니다.
원리: 큰 수의 법칙을 기반으로 합니다. 충분한 샘플(경험)을 통해 얻은 반환값($G_t$)의 평균이 수학적 **기대값**에 수렴합니다.(확률분포와 같아집니다.)

* 샘플 데이터를 얻을 떄마다 평균을 구해야 할 때는 증분 방식이 더 효율적입니다.
```python
import numpy as np

def sample(dices=2):
    x = 0
    for _ in range(dices):
        x += np.random.choice([1, 2, 3, 4, 5, 6])
    return x

trial = 1000
V, n = 0, 0

for _ in range(trial):
    s = sample()
    n += 1
    V += (s - V) / n
    print(V)
```
---
### 5.2 몬테카를로법으로 정책 평가하기

상태 $s$의 가치 함수 $\mathbf{V}_{\pi}(s)$를 $N$번의 에피소드 경험을 통해 얻은 반환값 $G$들의 평균으로 추정하는 몬테카를로법의 수식은 다음과 같이 간략하게 표현할 수 있습니다.
    $$\mathbf{V}_{\pi}(s) \approx \frac{\mathbf{G^{(1)}} + \mathbf{G^{(2)}} + \mathbf{\cdots} + \mathbf{G^{(N)}}}{N}$$

실제 경험을 통해 얻은 수익들의 평균이 그 상태의 미래 기대 수익이 된다는 큰 수의 법칙 원리를 나타냅니다.

#### 반환값($G_t$)의 계산
특정 시점 $t$ 이후부터 에피소드가 끝날 때까지 얻는 **할인된 보상($R$)의 총합**입니다.

    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$

---
### 5.3 몬테카를법 구현

#### 에이전트 클래스 구현

에이전트가 행동을 선택하게 하는 것이 조건입니다.

```python

class RandomAgent:
    def __init__(self):
        # --- 몬테카를로법 관련 기본 설정 ---
        self.gamma = 0.9                     # 할인율: 미래 보상의 가치를 현재로 환산하는 비율.
        self.action_size = 4                 # 환경에서 가능한 행동의 개수 (예: 상, 하, 좌, 우).

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25} 
        # 정책(π): 무작위 행동을 할 확률 분포.
        self.pi = defaultdict(lambda: random_actions) 
        
        # 상태 가치 함수(V(s)): 각 상태의 기대 수익을 저장. 초기값은 0.
        self.V = defaultdict(lambda: 0)
        
        # 증분방식으로 수익의 평균을 구할 때 사용.
        self.cnts = defaultdict(lambda: 0) 
        
        # 에이전트가 실제로 하나의 에피소드를 행동하는 동안 얻는 동안의 경험 (상태, 행동, 보상)을 저장.
        self.memory = []

    def get_action(self, state):
        # 현재 정책(self.pi)에 따라 행동을 선택.
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        # 확률(p=probs)에 기반하여 행동을 무작위로 샘플링하여 반환.
        return np.random.choice(actions, p=probs)

    def add(self, state, action, reward):
        # 환경과 상호작용 후 얻은 경험을 메모리에 추가. 목표지점의 가치함수는 항상 0이기 때문에 마지막 상태는 저장되지 않는 점을 유의.
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        # 하나의 에피소드가 끝난 후, 다음 에피소드를 위해 메모리를 초기화.
        self.memory.clear()

    def eval(self):
        # 몬테카를로 정책 평가의 핵심: 에피소드 종료 후 가치 함수(V) 갱신.
        G = 0 # 반환값(Return, Gt)을 계산하기 위한 초기값 설정.
        
        # 1. 에피소드 경험을 역순으로 순회하며 각 생태에서 얻은 수익을 계산.
        for data in reversed(self.memory):
            state, action, reward = data
            
            # Gt = R_t+1 + gamma * G_t+1 공식을 사용하여 G를 갱신.
            G = self.gamma * G + reward
            
            # 2. 가치 함수(V(s))를 갱신.
            
            # 해당 상태의 방문 횟수를 1 증가 (N(s) 갱신).
            self.cnts[state] += 1
            
            # V(s) 갱신: V_new = V_old + 1/N * (G - V_old) (수익 이동 평균)
            # 이를 통해 V[state]는 G 값들의 누적 평균으로 수렴하게 됨.
            self.V[state] += (G - self.V[state]) / self.cnts[state]
```
#### 몬테카를로법 실행

GridWorld 클래스를 연동하여 실행해봅시다

```python
env = GridWorld()
agent = RandomAgent()

episodes = 1000
for episode in range(episodes):
    state = env.reset()
    agent.reset()

    while True:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)

        agent.add(state, action, reward)
        if done:
            agent.eval()
            break

        state = next_state

# 가치함수 시각화
env.render_v(agent.V)
```
총 1000번의 에피소드를 실행하였고, 에이전트에게 먼저 행동하게 하고 그 결과로 얻은 샘플 데이터를 기록합니다.
샘플데이터를 이용하여 몬테카를로법으로 가치함수를 갱신합니다.
몬테카를로법을 이용하면 **환경모델을 몰라도 정책평가를 제대로 할 수 있습니다.**

가치함수를 시각화한 결과는 아래와 같습니다.

<img width="520" height="394" alt="image" src="https://github.com/user-attachments/assets/372c39b2-f953-4db7-9318-c93647aa26be" />

---
## 5.4 몬테카를로법으로 정책 제어하기

최적 정책은 평가와 개선을 번갈아 반복하여 얻습니다. 평가단계에서는 가치함수를 얻고, 개선 단계에서는 가치함수를 탐욕화하여 정책을 개선합니다.
이 과정을 번갈아 반복하여 최적 정책에 가까워집니다.

#### 행동 가치 함수($Q$)의 사용필요성:
몬테카를로법의 모델-프리 환경에서는 $V(s)$만으로는 최적의 행동을 알 수 없기 때문에, 특정 행동 $a$를 했을 때의 가치인 **$Q(s, a)$**를 추정하여 정책을 개선합니다.
 $\epsilon$-Greedy 정책목적: **탐색**과 **활용**의 균형을 유지하여 최적 정책($\pi^*$)을 찾습니다.

```python

def greedy_probs(Q, state, epsilon=0, action_size=4):
    qs = [Q[(state, action)] for action in range(action_size)]
    max_action = np.argmax(qs)

    base_prob = epsilon / action_size
    action_probs = {action: base_prob for action in range(action_size)}  #{0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}
    action_probs[max_action] += (1 - epsilon)
    return action_probs


class McAgent:
    def __init__(self):
        self.gamma = 0.9
        self.action_size = 4

        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}
        self.pi = defaultdict(lambda: random_actions)
        self.Q = defaultdict(lambda: 0)
        self.memory = []

    def get_action(self, state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions, p=probs)

    def add(self, state, action, reward):
        data = (state, action, reward)
        self.memory.append(data)

    def reset(self):
        self.memory.clear()

    def update(self):
        G = 0
        for data in reversed(self.memory):
            state, action, reward = data
            G = self.gamma * G + reward
            key = (state, action)
           self.cnts[key] += 1
            self.Q[key] += (G - self.Q[key]) / self.cnts[key]
            self.pi[state] = greedy_probs(self.Q, state)
```

구분RandomAgent (정책 평가)McAgent (정책 제어)강화 학습 목표주어진 정책의 가치(V)를 평가**최적 정책($\pi^*$)**을 습득/제어주요 추정 함수상태 가치 함수 V(s)행동 가치 함수 Q(s, a)학습 방식정책 평가(Policy Evaluation)정책 제어(Policy Control)정책 ($\pi$)고정됨 (균등 무작위 정책)변함 ($\epsilon$-Greedy 기반으로 Q에 맞춰 개선)갱신 대상V 테이블 및 상태 방문 횟수 N(s)Q 테이블 및 정책 $\pi$갱신 데이터 키state(state, action) 쌍 (key)가치 갱신 공식V(s)를 N(s) 기반 평균으로 갱신Q(s, a)를 N(s, a) 기반 평균으로 갱신정책 개선 단계없음있음 (greedy_probs 함수로 Q를 반영)
